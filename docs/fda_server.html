<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>fda_server API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fda_server</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import copy
from collections import OrderedDict

import matplotlib.pyplot as plt
import numpy as np
import torch
from PIL import Image

import datasets.ss_transforms as sstr
from utils.style_transfer import StyleAugment
from utils.utils import get_save_string

class FdaServer:
    &#34;&#34;&#34;
    A class representing the Server where FDA (Fourier Domain Adaptation) is used.

    The FDA server is responsible for coordinating the training process of the federated learning system.

    Args:
        `args`: An object containing the arguments and configuration for the FDA server.\n
        `source_dataset`: The source dataset used for domain adaptation.\n
        `train_clients`: A list of training clients.\n
        `test_clients`: A list of test clients.\n
        `model`: The initial source model.\n
        `metrics`: A dictionary containing evaluation metrics.\n
        `valid` (bool): Whether to include validation during training. Default is False.\n
        `valid_clients`: A list of validation clients. Required if `valid` is True. Default is None.
    &#34;&#34;&#34;
    def __init__(self, args, source_dataset, train_clients, test_clients, model, metrics, valid=False, valid_clients=None):
        self.args = args
        self.source_dataset = source_dataset
        self.train_clients = train_clients
        self.test_clients = test_clients
        self.validation_clients = valid_clients
        self.source_model = model
        self.metrics = metrics
        self.activate_val = valid
        self.model_params_dict = copy.deepcopy(self.source_model.state_dict())

        self.teacher_model = None
        self.student_model = None
        self.teacher_counter = 0

        # Style transfer
        self.styleaug = StyleAugment(args.n_images_per_style, args.fda_L, args.fda_size, b=args.fda_b) 
        
        if self.args.resume or (not args.load and not args.load_from):
            self.extract_styles()
        
    def select_clients(self, seed=None):
        &#34;&#34;&#34;
        Selects a subset of clients for a training round.

        Args:
            `seed` (int): Random seed for client selection. Default is None.

        Returns:
            numpy.ndarray: An array of selected clients for the training round.
        &#34;&#34;&#34;
        num_clients = min(self.args.clients_per_round, len(self.train_clients))
        if seed:
            np.random.seed(seed)
        else:
            np.random.seed(self.args.seed)
        return np.random.choice(self.train_clients, num_clients, replace=False)

    def extract_styles(self):
        &#34;&#34;&#34;
        Extracts styles from the training clients and adds them to the style augmenter.
        &#34;&#34;&#34;
        print(f&#34;Extracting the style from {len(self.train_clients)} clients.&#34;)
        for c in self.train_clients:
            self.styleaug.add_style(c.dataset)
    
    def train_source(self):
        &#34;&#34;&#34;
        Trains the source model on the source dataset.
        &#34;&#34;&#34;
        retrain_error=False

        if self.args.load or self.args.resume or self.args.load_from:
            # If specified a custom name for the saved model load the path
            if self.args.load_from:
                pth = self.args.load_from
            else:
                pth = f&#34;models/checkpoints/{get_save_string(self.args, True)}_checkpoint.pth&#34; if self.args.chp else f&#34;models/{get_save_string(self.args, True)}_best_model.pth&#34;
            try:
                saved_params = torch.load(pth)
                self.model_params_dict = saved_params
                self.source_model.load_state_dict(saved_params)
                self.source_model.eval()
                to_print = &#34; from checkpoints.&#34; if self.args.chp else &#34;.&#34;
                print(f&#34;Source model loaded{to_print}&#34;)
            except:
                print(&#34;The checkpoint for this model does not exist. The model will be retrained.&#34;)
                retrain_error=True
                
        if (not self.args.load and not self.args.load_from) or self.args.resume or retrain_error:
            _, model_dict = self.train_round_source(self.source_dataset)
            self.model_params_dict = model_dict
            self.source_model.load_state_dict(self.model_params_dict)

        if self.args.save:
            print(&#34;Saving training source...&#34;)
            torch.save(self.model_params_dict, f&#39;models/{get_save_string(self.args, True)}_best_model.pth&#39;)

    def train_round_source(self, client):
        &#34;&#34;&#34;
        Trains a single round of the source model.

        Args:
            `client`: A client object containing the source dataset.

        Returns:
            tuple: (len_dataset, dict_params). Model updates gathered from the client to be aggregated.
        &#34;&#34;&#34;
        print(f&#34;\n Training on source dataset starting.. Num. of samples: {len(client[0].dataset)}&#34;)
        # Apply style augmentation
        client[0].set_set_style_tf_fn(self.styleaug)
        #Update parameters of the client model
        client[0].model.load_state_dict(self.model_params_dict)
        # Temp line. setup train
        self.metrics[&#34;eval_train&#34;].reset()  
        update = client[0].train(self.metrics[&#34;eval_train&#34;], [self.test_clients[0].test_loader])
        return update
    
    def train_round(self, clients):
        &#34;&#34;&#34;
        Trains the model with the datasets of multiple clients in a federated training round.

        Args:
            `clients`: A list of clients to train.

        Returns:
            list[tuple]: [(len_dataset, state_dictionary)]. Model updates gathered from the client to be aggregated.
        &#34;&#34;&#34;
        updates = []
        
        # Increment the teacher steps counter
        self.teacher_counter += 1
        if (self.teacher_counter % self.args.teacher_step) == 0:
            # Update teacher
            print(f&#34;Updating teacher at step {self.teacher_counter}&#34;)
            self.teacher_model.load_state_dict(copy.deepcopy(self.model_params_dict))

        # Test client augmetation
        for i, c in enumerate(clients):
            print(f&#34;Client: {c.name} turn: Num. of samples: {len(c.dataset)}, ({i+1}/{len(clients)})&#34;)
            # Load student_model params
            c.model.load_state_dict(copy.deepcopy(self.student_model.state_dict()))
            # Set teacher model on the client
            c.set_teacher(self.teacher_model)
            # Reset counter for early stop
            c.early_stopper.reset_counter()
            # Start the train on the client
            update = c.train()
            updates.append(update)
        return updates

    def aggregate(self, updates):
        &#34;&#34;&#34;
        Aggregates the model updates received from the clients using FedAvg.

        Args:
            `updates`: Model updates received from the clients. List of tuples.

        Returns:
            OrderedDict: Aggregated model parameters.
        &#34;&#34;&#34;
        total_weight = 0.
        base = OrderedDict()

        # Numerator of weighted average
        for (client_samples, client_model) in updates:
            total_weight += client_samples
            for key, value in client_model.items():
                if key in base:
                    base[key] += client_samples * value.type(torch.FloatTensor)
                else:
                    base[key] = client_samples * value.type(torch.FloatTensor)

        averaged_sol_n = copy.deepcopy(self.model_params_dict)

        # Denominator of weighted average
        for key, value in base.items():
            if total_weight != 0:
                averaged_sol_n[key] = value.to(&#39;cuda&#39;) / total_weight

        # Return weighted average
        return averaged_sol_n

    def train(self):
        &#34;&#34;&#34;
        This method orchestrates the training the evals and tests at rounds level.
        &#34;&#34;&#34;

        num_rounds = self.args.num_rounds
        if self.args.centr:
            num_rounds = 1

        # Centralized train on source dataset
        self.train_source()
        # Validate on the train source
        self.eval_source()
        # Validate on train partition of the tarhet
        self.eval_train()
        # Validate on the test sets
        self.test()

        # Setup teacher and student
        self.teacher_model = copy.deepcopy(self.source_model)
        self.student_model = copy.deepcopy(self.source_model)

        # Load an existing checkpoint and resume training logic
        if self.args.load or self.args.resume:
            pth = f&#34;models/checkpoints/{get_save_string(self.args, False)}_checkpoint.pth&#34; if self.args.chp else f&#34;models/{get_save_string(self.args, False)}_best_model.pth&#34;
            try:
                saved_params = torch.load(pth)
                self.model_params_dict = saved_params
                self.source_model.load_state_dict(saved_params)
                self.source_model.eval()
                to_print = &#34; from checkpoints.&#34; if self.args.chp else &#34;.&#34;
                print(f&#34;FDA full model loaded {to_print}&#34;)
            except:
                print(&#34;The checkpoint for this model does not exist. The model will be retrained.&#34;)
                retrain_error=True
        
        # DISTRIBUTED TRAINING
        if (not self.args.load) or self.args.resume or retrain_error or self.args.load_from:
            # Start of distributed train
            for r in range(num_rounds):                
                print(&#34;------------------&#34;)
                print(f&#34;Round {r+1}/{num_rounds} started.&#34;)
                print(&#34;------------------&#34;)

                # Select random subset of clients
                chosen_clients = self.select_clients(seed=r)
                
                # Train a round
                updates = self.train_round(chosen_clients)

                # Aggregate the parameters
                self.model_params_dict = self.aggregate(updates)

                # Save in the student model the aggregated weights
                self.student_model.load_state_dict(copy.deepcopy(self.model_params_dict))

        if self.args.save:
            print(&#34;Saving full FDA model...&#34;)
            torch.save(self.model_params_dict, f&#39;models/{get_save_string(self.args, False)}_best_model.pth&#39;)

        # Validate on the train source
        self.eval_source()
        # Validate on train partition of the tarhet
        self.eval_train()
        # Validate on the test sets
        self.test()

    def eval_source(self):
        &#34;&#34;&#34;
        This method handles the evaluation on the source client
        &#34;&#34;&#34;
        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on SOURCE DATASET started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;eval_train&#34;].reset()

        self.source_dataset[0].model.load_state_dict(copy.deepcopy(self.model_params_dict))
        self.source_dataset[0].test(self.metrics[&#34;eval_train&#34;])

        res=self.metrics[&#34;eval_train&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    def eval_train(self):
        &#34;&#34;&#34;
        This method handles the evaluation on the train clients
        &#34;&#34;&#34;
        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on TARGET DATASET started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;eval_train&#34;].reset()

        for c in self.train_clients:
            c.model.load_state_dict(copy.deepcopy(self.model_params_dict))
            c.test(self.metrics[&#34;eval_train&#34;])

        res=self.metrics[&#34;eval_train&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    def test(self):
        &#34;&#34;&#34;
            This method handles the test on the test clients
        &#34;&#34;&#34;
        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on SAME DOMAIN DATA started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;test_same_dom&#34;].reset()

        self.test_clients[0].model.load_state_dict(copy.deepcopy(self.model_params_dict))
        self.test_clients[0].test(self.metrics[&#34;test_same_dom&#34;])

        res=self.metrics[&#34;test_same_dom&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on DIFFERENT DOMAIN DATA started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;test_diff_dom&#34;].reset()

        self.test_clients[1].model.load_state_dict(copy.deepcopy(self.model_params_dict))
        self.test_clients[1].test(self.metrics[&#34;test_diff_dom&#34;])

        res=self.metrics[&#34;test_diff_dom&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    def predict(self, image_path):
        &#34;&#34;&#34;
        Handles the the prediction. Outputs an image in the root directory.
        Args: 
            `image_path`: path to the image to predict.
        &#34;&#34;&#34;
        def get_outputs(images, labels=None, test=False):
            if self.args.model == &#39;deeplabv3_mobilenetv2&#39;:
                return self.source_model(images)[&#39;out&#39;]
            if self.args.model in [&#39;resnet18&#39;,]:
                return self.source_model(images)
            if self.args.model == &#39;segformer&#39;:
                logits = self.source_model(images).logits
                outputs = torch.nn.functional.interpolate(
                        logits, 
                        size=images.shape[-2:], 
                        mode=&#34;bilinear&#34;, 
                        align_corners=False
                )
                return outputs
            if self.args.model == &#39;bisenetv2&#39;:
                outputs = self.source_model(images, test=test)
                return outputs
            
        # Load and preprocess the input image
        input_image = Image.open(image_path)

        # Apply necessary transformations
        transforms= sstr.Compose([
            sstr.ToTensor(),
            sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        # Add batch dimension
        input_tensor = transforms(input_image).unsqueeze(0)  
        input_tensor = input_tensor.cuda()
        self.source_model.eval()

        # Perform inference
        with torch.no_grad():
            output = get_outputs(input_tensor, test=True)  # Get the output logits
        output = output.squeeze(0).cpu().numpy()
    
        normalized_output = (output - output.min()) / (output.max() - output.min())

        predicted_labels = np.argmax(normalized_output, axis=0)

        # Get colormap
        colormap = plt.cm.get_cmap(&#39;tab20&#39;, predicted_labels.max() + 1)

        # Create the predicted image with colors
        predicted_image = Image.fromarray((colormap(predicted_labels) * 255).astype(np.uint8))
        
        # Save the predicted image
        if self.args.dataset != &#34;loveda&#34;:
            class_names = [&#34;road&#34;, &#34;sidewalk&#34;, &#34;building&#34;, &#34;wall&#34;, &#34;fence&#34;, &#34;pole&#34;, &#34;traffic light&#34;, &#34;traffic sign&#34;, &#34;vegatation&#34;, &#34;terrain&#34;, &#34;sky&#34;, &#34;person&#34;, &#34;rider&#34;, &#34;car&#34;, &#34;motorcycle&#34;, &#34;bicycle&#34;]
        else:
            class_names = [&#34;DC&#34;, &#34;background&#34;, &#34;building&#34;, &#34;road&#34;, &#34;water&#34;, &#34;barren&#34;, &#34;forest&#34;, &#34;agriculture&#34;]

        # Create a legend
        legend_elements = [plt.Rectangle((0, 0), 1, 1, color=colormap(i)) for i in range(len(class_names))]

        # Create a figure and axes
        _, ax = plt.subplots()

       # Display the predicted image
        ax.imshow(np.array(input_image))

        if self.args.dataset == &#34;loveda&#34;:
            ax.imshow(predicted_image, alpha=0.4)
        else:  
            ax.imshow(predicted_image, alpha=0.7)
        ax.axis(&#39;off&#39;)

        # Create the legend outside the image
        legend = ax.legend(legend_elements, class_names, loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5))

        # Adjust the positioning and appearance of the legend
        legend.set_title(&#39;Legend&#39;)
        frame = legend.get_frame()
        frame.set_edgecolor(&#39;black&#39;)
        frame.set_facecolor(&#39;white&#39;)

        # Save the figure
        plt.savefig(&#39;fda_image_fin.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)

        # Private method to get the different fda images 
        #self.__image_fda()

    def __image_fda(self):
        # Temp function to save image of the style transfer (FDA)
        # Load and preprocess the input image
        dataset = self.source_dataset[0].dataset
        dataset.return_unprocessed_image = True
        input_image = self.styleaug.apply_style(dataset[4])
        # Create the predicted image with colors
    
        fig, ax = plt.subplots()

        # Display the predicted image
        ax.imshow(input_image)
        ax.axis(&#39;off&#39;)
        # Save the figure
        plt.savefig(&#39;fda_transform.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fda_server.FdaServer"><code class="flex name class">
<span>class <span class="ident">FdaServer</span></span>
<span>(</span><span>args, source_dataset, train_clients, test_clients, model, metrics, valid=False, valid_clients=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A class representing the Server where FDA (Fourier Domain Adaptation) is used.</p>
<p>The FDA server is responsible for coordinating the training process of the federated learning system.</p>
<h2 id="args">Args</h2>
<p><code>args</code>: An object containing the arguments and configuration for the FDA server.</p>
<p><code>source_dataset</code>: The source dataset used for domain adaptation.</p>
<p><code>train_clients</code>: A list of training clients.</p>
<p><code>test_clients</code>: A list of test clients.</p>
<p><code>model</code>: The initial source model.</p>
<p><code>metrics</code>: A dictionary containing evaluation metrics.</p>
<p><code>valid</code> (bool): Whether to include validation during training. Default is False.</p>
<p><code>valid_clients</code>: A list of validation clients. Required if <code>valid</code> is True. Default is None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FdaServer:
    &#34;&#34;&#34;
    A class representing the Server where FDA (Fourier Domain Adaptation) is used.

    The FDA server is responsible for coordinating the training process of the federated learning system.

    Args:
        `args`: An object containing the arguments and configuration for the FDA server.\n
        `source_dataset`: The source dataset used for domain adaptation.\n
        `train_clients`: A list of training clients.\n
        `test_clients`: A list of test clients.\n
        `model`: The initial source model.\n
        `metrics`: A dictionary containing evaluation metrics.\n
        `valid` (bool): Whether to include validation during training. Default is False.\n
        `valid_clients`: A list of validation clients. Required if `valid` is True. Default is None.
    &#34;&#34;&#34;
    def __init__(self, args, source_dataset, train_clients, test_clients, model, metrics, valid=False, valid_clients=None):
        self.args = args
        self.source_dataset = source_dataset
        self.train_clients = train_clients
        self.test_clients = test_clients
        self.validation_clients = valid_clients
        self.source_model = model
        self.metrics = metrics
        self.activate_val = valid
        self.model_params_dict = copy.deepcopy(self.source_model.state_dict())

        self.teacher_model = None
        self.student_model = None
        self.teacher_counter = 0

        # Style transfer
        self.styleaug = StyleAugment(args.n_images_per_style, args.fda_L, args.fda_size, b=args.fda_b) 
        
        if self.args.resume or (not args.load and not args.load_from):
            self.extract_styles()
        
    def select_clients(self, seed=None):
        &#34;&#34;&#34;
        Selects a subset of clients for a training round.

        Args:
            `seed` (int): Random seed for client selection. Default is None.

        Returns:
            numpy.ndarray: An array of selected clients for the training round.
        &#34;&#34;&#34;
        num_clients = min(self.args.clients_per_round, len(self.train_clients))
        if seed:
            np.random.seed(seed)
        else:
            np.random.seed(self.args.seed)
        return np.random.choice(self.train_clients, num_clients, replace=False)

    def extract_styles(self):
        &#34;&#34;&#34;
        Extracts styles from the training clients and adds them to the style augmenter.
        &#34;&#34;&#34;
        print(f&#34;Extracting the style from {len(self.train_clients)} clients.&#34;)
        for c in self.train_clients:
            self.styleaug.add_style(c.dataset)
    
    def train_source(self):
        &#34;&#34;&#34;
        Trains the source model on the source dataset.
        &#34;&#34;&#34;
        retrain_error=False

        if self.args.load or self.args.resume or self.args.load_from:
            # If specified a custom name for the saved model load the path
            if self.args.load_from:
                pth = self.args.load_from
            else:
                pth = f&#34;models/checkpoints/{get_save_string(self.args, True)}_checkpoint.pth&#34; if self.args.chp else f&#34;models/{get_save_string(self.args, True)}_best_model.pth&#34;
            try:
                saved_params = torch.load(pth)
                self.model_params_dict = saved_params
                self.source_model.load_state_dict(saved_params)
                self.source_model.eval()
                to_print = &#34; from checkpoints.&#34; if self.args.chp else &#34;.&#34;
                print(f&#34;Source model loaded{to_print}&#34;)
            except:
                print(&#34;The checkpoint for this model does not exist. The model will be retrained.&#34;)
                retrain_error=True
                
        if (not self.args.load and not self.args.load_from) or self.args.resume or retrain_error:
            _, model_dict = self.train_round_source(self.source_dataset)
            self.model_params_dict = model_dict
            self.source_model.load_state_dict(self.model_params_dict)

        if self.args.save:
            print(&#34;Saving training source...&#34;)
            torch.save(self.model_params_dict, f&#39;models/{get_save_string(self.args, True)}_best_model.pth&#39;)

    def train_round_source(self, client):
        &#34;&#34;&#34;
        Trains a single round of the source model.

        Args:
            `client`: A client object containing the source dataset.

        Returns:
            tuple: (len_dataset, dict_params). Model updates gathered from the client to be aggregated.
        &#34;&#34;&#34;
        print(f&#34;\n Training on source dataset starting.. Num. of samples: {len(client[0].dataset)}&#34;)
        # Apply style augmentation
        client[0].set_set_style_tf_fn(self.styleaug)
        #Update parameters of the client model
        client[0].model.load_state_dict(self.model_params_dict)
        # Temp line. setup train
        self.metrics[&#34;eval_train&#34;].reset()  
        update = client[0].train(self.metrics[&#34;eval_train&#34;], [self.test_clients[0].test_loader])
        return update
    
    def train_round(self, clients):
        &#34;&#34;&#34;
        Trains the model with the datasets of multiple clients in a federated training round.

        Args:
            `clients`: A list of clients to train.

        Returns:
            list[tuple]: [(len_dataset, state_dictionary)]. Model updates gathered from the client to be aggregated.
        &#34;&#34;&#34;
        updates = []
        
        # Increment the teacher steps counter
        self.teacher_counter += 1
        if (self.teacher_counter % self.args.teacher_step) == 0:
            # Update teacher
            print(f&#34;Updating teacher at step {self.teacher_counter}&#34;)
            self.teacher_model.load_state_dict(copy.deepcopy(self.model_params_dict))

        # Test client augmetation
        for i, c in enumerate(clients):
            print(f&#34;Client: {c.name} turn: Num. of samples: {len(c.dataset)}, ({i+1}/{len(clients)})&#34;)
            # Load student_model params
            c.model.load_state_dict(copy.deepcopy(self.student_model.state_dict()))
            # Set teacher model on the client
            c.set_teacher(self.teacher_model)
            # Reset counter for early stop
            c.early_stopper.reset_counter()
            # Start the train on the client
            update = c.train()
            updates.append(update)
        return updates

    def aggregate(self, updates):
        &#34;&#34;&#34;
        Aggregates the model updates received from the clients using FedAvg.

        Args:
            `updates`: Model updates received from the clients. List of tuples.

        Returns:
            OrderedDict: Aggregated model parameters.
        &#34;&#34;&#34;
        total_weight = 0.
        base = OrderedDict()

        # Numerator of weighted average
        for (client_samples, client_model) in updates:
            total_weight += client_samples
            for key, value in client_model.items():
                if key in base:
                    base[key] += client_samples * value.type(torch.FloatTensor)
                else:
                    base[key] = client_samples * value.type(torch.FloatTensor)

        averaged_sol_n = copy.deepcopy(self.model_params_dict)

        # Denominator of weighted average
        for key, value in base.items():
            if total_weight != 0:
                averaged_sol_n[key] = value.to(&#39;cuda&#39;) / total_weight

        # Return weighted average
        return averaged_sol_n

    def train(self):
        &#34;&#34;&#34;
        This method orchestrates the training the evals and tests at rounds level.
        &#34;&#34;&#34;

        num_rounds = self.args.num_rounds
        if self.args.centr:
            num_rounds = 1

        # Centralized train on source dataset
        self.train_source()
        # Validate on the train source
        self.eval_source()
        # Validate on train partition of the tarhet
        self.eval_train()
        # Validate on the test sets
        self.test()

        # Setup teacher and student
        self.teacher_model = copy.deepcopy(self.source_model)
        self.student_model = copy.deepcopy(self.source_model)

        # Load an existing checkpoint and resume training logic
        if self.args.load or self.args.resume:
            pth = f&#34;models/checkpoints/{get_save_string(self.args, False)}_checkpoint.pth&#34; if self.args.chp else f&#34;models/{get_save_string(self.args, False)}_best_model.pth&#34;
            try:
                saved_params = torch.load(pth)
                self.model_params_dict = saved_params
                self.source_model.load_state_dict(saved_params)
                self.source_model.eval()
                to_print = &#34; from checkpoints.&#34; if self.args.chp else &#34;.&#34;
                print(f&#34;FDA full model loaded {to_print}&#34;)
            except:
                print(&#34;The checkpoint for this model does not exist. The model will be retrained.&#34;)
                retrain_error=True
        
        # DISTRIBUTED TRAINING
        if (not self.args.load) or self.args.resume or retrain_error or self.args.load_from:
            # Start of distributed train
            for r in range(num_rounds):                
                print(&#34;------------------&#34;)
                print(f&#34;Round {r+1}/{num_rounds} started.&#34;)
                print(&#34;------------------&#34;)

                # Select random subset of clients
                chosen_clients = self.select_clients(seed=r)
                
                # Train a round
                updates = self.train_round(chosen_clients)

                # Aggregate the parameters
                self.model_params_dict = self.aggregate(updates)

                # Save in the student model the aggregated weights
                self.student_model.load_state_dict(copy.deepcopy(self.model_params_dict))

        if self.args.save:
            print(&#34;Saving full FDA model...&#34;)
            torch.save(self.model_params_dict, f&#39;models/{get_save_string(self.args, False)}_best_model.pth&#39;)

        # Validate on the train source
        self.eval_source()
        # Validate on train partition of the tarhet
        self.eval_train()
        # Validate on the test sets
        self.test()

    def eval_source(self):
        &#34;&#34;&#34;
        This method handles the evaluation on the source client
        &#34;&#34;&#34;
        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on SOURCE DATASET started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;eval_train&#34;].reset()

        self.source_dataset[0].model.load_state_dict(copy.deepcopy(self.model_params_dict))
        self.source_dataset[0].test(self.metrics[&#34;eval_train&#34;])

        res=self.metrics[&#34;eval_train&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    def eval_train(self):
        &#34;&#34;&#34;
        This method handles the evaluation on the train clients
        &#34;&#34;&#34;
        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on TARGET DATASET started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;eval_train&#34;].reset()

        for c in self.train_clients:
            c.model.load_state_dict(copy.deepcopy(self.model_params_dict))
            c.test(self.metrics[&#34;eval_train&#34;])

        res=self.metrics[&#34;eval_train&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    def test(self):
        &#34;&#34;&#34;
            This method handles the test on the test clients
        &#34;&#34;&#34;
        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on SAME DOMAIN DATA started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;test_same_dom&#34;].reset()

        self.test_clients[0].model.load_state_dict(copy.deepcopy(self.model_params_dict))
        self.test_clients[0].test(self.metrics[&#34;test_same_dom&#34;])

        res=self.metrics[&#34;test_same_dom&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

        print(&#34;------------------------------------&#34;)
        print(f&#34;Test on DIFFERENT DOMAIN DATA started.&#34;)
        print(&#34;------------------------------------&#34;)

        self.metrics[&#34;test_diff_dom&#34;].reset()

        self.test_clients[1].model.load_state_dict(copy.deepcopy(self.model_params_dict))
        self.test_clients[1].test(self.metrics[&#34;test_diff_dom&#34;])

        res=self.metrics[&#34;test_diff_dom&#34;].get_results()
        print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    def predict(self, image_path):
        &#34;&#34;&#34;
        Handles the the prediction. Outputs an image in the root directory.
        Args: 
            `image_path`: path to the image to predict.
        &#34;&#34;&#34;
        def get_outputs(images, labels=None, test=False):
            if self.args.model == &#39;deeplabv3_mobilenetv2&#39;:
                return self.source_model(images)[&#39;out&#39;]
            if self.args.model in [&#39;resnet18&#39;,]:
                return self.source_model(images)
            if self.args.model == &#39;segformer&#39;:
                logits = self.source_model(images).logits
                outputs = torch.nn.functional.interpolate(
                        logits, 
                        size=images.shape[-2:], 
                        mode=&#34;bilinear&#34;, 
                        align_corners=False
                )
                return outputs
            if self.args.model == &#39;bisenetv2&#39;:
                outputs = self.source_model(images, test=test)
                return outputs
            
        # Load and preprocess the input image
        input_image = Image.open(image_path)

        # Apply necessary transformations
        transforms= sstr.Compose([
            sstr.ToTensor(),
            sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        # Add batch dimension
        input_tensor = transforms(input_image).unsqueeze(0)  
        input_tensor = input_tensor.cuda()
        self.source_model.eval()

        # Perform inference
        with torch.no_grad():
            output = get_outputs(input_tensor, test=True)  # Get the output logits
        output = output.squeeze(0).cpu().numpy()
    
        normalized_output = (output - output.min()) / (output.max() - output.min())

        predicted_labels = np.argmax(normalized_output, axis=0)

        # Get colormap
        colormap = plt.cm.get_cmap(&#39;tab20&#39;, predicted_labels.max() + 1)

        # Create the predicted image with colors
        predicted_image = Image.fromarray((colormap(predicted_labels) * 255).astype(np.uint8))
        
        # Save the predicted image
        if self.args.dataset != &#34;loveda&#34;:
            class_names = [&#34;road&#34;, &#34;sidewalk&#34;, &#34;building&#34;, &#34;wall&#34;, &#34;fence&#34;, &#34;pole&#34;, &#34;traffic light&#34;, &#34;traffic sign&#34;, &#34;vegatation&#34;, &#34;terrain&#34;, &#34;sky&#34;, &#34;person&#34;, &#34;rider&#34;, &#34;car&#34;, &#34;motorcycle&#34;, &#34;bicycle&#34;]
        else:
            class_names = [&#34;DC&#34;, &#34;background&#34;, &#34;building&#34;, &#34;road&#34;, &#34;water&#34;, &#34;barren&#34;, &#34;forest&#34;, &#34;agriculture&#34;]

        # Create a legend
        legend_elements = [plt.Rectangle((0, 0), 1, 1, color=colormap(i)) for i in range(len(class_names))]

        # Create a figure and axes
        _, ax = plt.subplots()

       # Display the predicted image
        ax.imshow(np.array(input_image))

        if self.args.dataset == &#34;loveda&#34;:
            ax.imshow(predicted_image, alpha=0.4)
        else:  
            ax.imshow(predicted_image, alpha=0.7)
        ax.axis(&#39;off&#39;)

        # Create the legend outside the image
        legend = ax.legend(legend_elements, class_names, loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5))

        # Adjust the positioning and appearance of the legend
        legend.set_title(&#39;Legend&#39;)
        frame = legend.get_frame()
        frame.set_edgecolor(&#39;black&#39;)
        frame.set_facecolor(&#39;white&#39;)

        # Save the figure
        plt.savefig(&#39;fda_image_fin.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)

        # Private method to get the different fda images 
        #self.__image_fda()

    def __image_fda(self):
        # Temp function to save image of the style transfer (FDA)
        # Load and preprocess the input image
        dataset = self.source_dataset[0].dataset
        dataset.return_unprocessed_image = True
        input_image = self.styleaug.apply_style(dataset[4])
        # Create the predicted image with colors
    
        fig, ax = plt.subplots()

        # Display the predicted image
        ax.imshow(input_image)
        ax.axis(&#39;off&#39;)
        # Save the figure
        plt.savefig(&#39;fda_transform.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="fda_server.FdaServer.aggregate"><code class="name flex">
<span>def <span class="ident">aggregate</span></span>(<span>self, updates)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregates the model updates received from the clients using FedAvg.</p>
<h2 id="args">Args</h2>
<p><code>updates</code>: Model updates received from the clients. List of tuples.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>OrderedDict</code></dt>
<dd>Aggregated model parameters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def aggregate(self, updates):
    &#34;&#34;&#34;
    Aggregates the model updates received from the clients using FedAvg.

    Args:
        `updates`: Model updates received from the clients. List of tuples.

    Returns:
        OrderedDict: Aggregated model parameters.
    &#34;&#34;&#34;
    total_weight = 0.
    base = OrderedDict()

    # Numerator of weighted average
    for (client_samples, client_model) in updates:
        total_weight += client_samples
        for key, value in client_model.items():
            if key in base:
                base[key] += client_samples * value.type(torch.FloatTensor)
            else:
                base[key] = client_samples * value.type(torch.FloatTensor)

    averaged_sol_n = copy.deepcopy(self.model_params_dict)

    # Denominator of weighted average
    for key, value in base.items():
        if total_weight != 0:
            averaged_sol_n[key] = value.to(&#39;cuda&#39;) / total_weight

    # Return weighted average
    return averaged_sol_n</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.eval_source"><code class="name flex">
<span>def <span class="ident">eval_source</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This method handles the evaluation on the source client</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval_source(self):
    &#34;&#34;&#34;
    This method handles the evaluation on the source client
    &#34;&#34;&#34;
    print(&#34;------------------------------------&#34;)
    print(f&#34;Test on SOURCE DATASET started.&#34;)
    print(&#34;------------------------------------&#34;)

    self.metrics[&#34;eval_train&#34;].reset()

    self.source_dataset[0].model.load_state_dict(copy.deepcopy(self.model_params_dict))
    self.source_dataset[0].test(self.metrics[&#34;eval_train&#34;])

    res=self.metrics[&#34;eval_train&#34;].get_results()
    print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.eval_train"><code class="name flex">
<span>def <span class="ident">eval_train</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This method handles the evaluation on the train clients</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval_train(self):
    &#34;&#34;&#34;
    This method handles the evaluation on the train clients
    &#34;&#34;&#34;
    print(&#34;------------------------------------&#34;)
    print(f&#34;Test on TARGET DATASET started.&#34;)
    print(&#34;------------------------------------&#34;)

    self.metrics[&#34;eval_train&#34;].reset()

    for c in self.train_clients:
        c.model.load_state_dict(copy.deepcopy(self.model_params_dict))
        c.test(self.metrics[&#34;eval_train&#34;])

    res=self.metrics[&#34;eval_train&#34;].get_results()
    print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.extract_styles"><code class="name flex">
<span>def <span class="ident">extract_styles</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts styles from the training clients and adds them to the style augmenter.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_styles(self):
    &#34;&#34;&#34;
    Extracts styles from the training clients and adds them to the style augmenter.
    &#34;&#34;&#34;
    print(f&#34;Extracting the style from {len(self.train_clients)} clients.&#34;)
    for c in self.train_clients:
        self.styleaug.add_style(c.dataset)</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, image_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Handles the the prediction. Outputs an image in the root directory.
Args:
<code>image_path</code>: path to the image to predict.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, image_path):
    &#34;&#34;&#34;
    Handles the the prediction. Outputs an image in the root directory.
    Args: 
        `image_path`: path to the image to predict.
    &#34;&#34;&#34;
    def get_outputs(images, labels=None, test=False):
        if self.args.model == &#39;deeplabv3_mobilenetv2&#39;:
            return self.source_model(images)[&#39;out&#39;]
        if self.args.model in [&#39;resnet18&#39;,]:
            return self.source_model(images)
        if self.args.model == &#39;segformer&#39;:
            logits = self.source_model(images).logits
            outputs = torch.nn.functional.interpolate(
                    logits, 
                    size=images.shape[-2:], 
                    mode=&#34;bilinear&#34;, 
                    align_corners=False
            )
            return outputs
        if self.args.model == &#39;bisenetv2&#39;:
            outputs = self.source_model(images, test=test)
            return outputs
        
    # Load and preprocess the input image
    input_image = Image.open(image_path)

    # Apply necessary transformations
    transforms= sstr.Compose([
        sstr.ToTensor(),
        sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Add batch dimension
    input_tensor = transforms(input_image).unsqueeze(0)  
    input_tensor = input_tensor.cuda()
    self.source_model.eval()

    # Perform inference
    with torch.no_grad():
        output = get_outputs(input_tensor, test=True)  # Get the output logits
    output = output.squeeze(0).cpu().numpy()

    normalized_output = (output - output.min()) / (output.max() - output.min())

    predicted_labels = np.argmax(normalized_output, axis=0)

    # Get colormap
    colormap = plt.cm.get_cmap(&#39;tab20&#39;, predicted_labels.max() + 1)

    # Create the predicted image with colors
    predicted_image = Image.fromarray((colormap(predicted_labels) * 255).astype(np.uint8))
    
    # Save the predicted image
    if self.args.dataset != &#34;loveda&#34;:
        class_names = [&#34;road&#34;, &#34;sidewalk&#34;, &#34;building&#34;, &#34;wall&#34;, &#34;fence&#34;, &#34;pole&#34;, &#34;traffic light&#34;, &#34;traffic sign&#34;, &#34;vegatation&#34;, &#34;terrain&#34;, &#34;sky&#34;, &#34;person&#34;, &#34;rider&#34;, &#34;car&#34;, &#34;motorcycle&#34;, &#34;bicycle&#34;]
    else:
        class_names = [&#34;DC&#34;, &#34;background&#34;, &#34;building&#34;, &#34;road&#34;, &#34;water&#34;, &#34;barren&#34;, &#34;forest&#34;, &#34;agriculture&#34;]

    # Create a legend
    legend_elements = [plt.Rectangle((0, 0), 1, 1, color=colormap(i)) for i in range(len(class_names))]

    # Create a figure and axes
    _, ax = plt.subplots()

   # Display the predicted image
    ax.imshow(np.array(input_image))

    if self.args.dataset == &#34;loveda&#34;:
        ax.imshow(predicted_image, alpha=0.4)
    else:  
        ax.imshow(predicted_image, alpha=0.7)
    ax.axis(&#39;off&#39;)

    # Create the legend outside the image
    legend = ax.legend(legend_elements, class_names, loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5))

    # Adjust the positioning and appearance of the legend
    legend.set_title(&#39;Legend&#39;)
    frame = legend.get_frame()
    frame.set_edgecolor(&#39;black&#39;)
    frame.set_facecolor(&#39;white&#39;)

    # Save the figure
    plt.savefig(&#39;fda_image_fin.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)

    # Private method to get the different fda images 
    #self.__image_fda()</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.select_clients"><code class="name flex">
<span>def <span class="ident">select_clients</span></span>(<span>self, seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Selects a subset of clients for a training round.</p>
<h2 id="args">Args</h2>
<p><code>seed</code> (int): Random seed for client selection. Default is None.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>An array of selected clients for the training round.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_clients(self, seed=None):
    &#34;&#34;&#34;
    Selects a subset of clients for a training round.

    Args:
        `seed` (int): Random seed for client selection. Default is None.

    Returns:
        numpy.ndarray: An array of selected clients for the training round.
    &#34;&#34;&#34;
    num_clients = min(self.args.clients_per_round, len(self.train_clients))
    if seed:
        np.random.seed(seed)
    else:
        np.random.seed(self.args.seed)
    return np.random.choice(self.train_clients, num_clients, replace=False)</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This method handles the test on the test clients</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test(self):
    &#34;&#34;&#34;
        This method handles the test on the test clients
    &#34;&#34;&#34;
    print(&#34;------------------------------------&#34;)
    print(f&#34;Test on SAME DOMAIN DATA started.&#34;)
    print(&#34;------------------------------------&#34;)

    self.metrics[&#34;test_same_dom&#34;].reset()

    self.test_clients[0].model.load_state_dict(copy.deepcopy(self.model_params_dict))
    self.test_clients[0].test(self.metrics[&#34;test_same_dom&#34;])

    res=self.metrics[&#34;test_same_dom&#34;].get_results()
    print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)

    print(&#34;------------------------------------&#34;)
    print(f&#34;Test on DIFFERENT DOMAIN DATA started.&#34;)
    print(&#34;------------------------------------&#34;)

    self.metrics[&#34;test_diff_dom&#34;].reset()

    self.test_clients[1].model.load_state_dict(copy.deepcopy(self.model_params_dict))
    self.test_clients[1].test(self.metrics[&#34;test_diff_dom&#34;])

    res=self.metrics[&#34;test_diff_dom&#34;].get_results()
    print(f&#39;Acc: {res[&#34;Overall Acc&#34;]}, Mean IoU: {res[&#34;Mean IoU&#34;]}&#39;)</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This method orchestrates the training the evals and tests at rounds level.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self):
    &#34;&#34;&#34;
    This method orchestrates the training the evals and tests at rounds level.
    &#34;&#34;&#34;

    num_rounds = self.args.num_rounds
    if self.args.centr:
        num_rounds = 1

    # Centralized train on source dataset
    self.train_source()
    # Validate on the train source
    self.eval_source()
    # Validate on train partition of the tarhet
    self.eval_train()
    # Validate on the test sets
    self.test()

    # Setup teacher and student
    self.teacher_model = copy.deepcopy(self.source_model)
    self.student_model = copy.deepcopy(self.source_model)

    # Load an existing checkpoint and resume training logic
    if self.args.load or self.args.resume:
        pth = f&#34;models/checkpoints/{get_save_string(self.args, False)}_checkpoint.pth&#34; if self.args.chp else f&#34;models/{get_save_string(self.args, False)}_best_model.pth&#34;
        try:
            saved_params = torch.load(pth)
            self.model_params_dict = saved_params
            self.source_model.load_state_dict(saved_params)
            self.source_model.eval()
            to_print = &#34; from checkpoints.&#34; if self.args.chp else &#34;.&#34;
            print(f&#34;FDA full model loaded {to_print}&#34;)
        except:
            print(&#34;The checkpoint for this model does not exist. The model will be retrained.&#34;)
            retrain_error=True
    
    # DISTRIBUTED TRAINING
    if (not self.args.load) or self.args.resume or retrain_error or self.args.load_from:
        # Start of distributed train
        for r in range(num_rounds):                
            print(&#34;------------------&#34;)
            print(f&#34;Round {r+1}/{num_rounds} started.&#34;)
            print(&#34;------------------&#34;)

            # Select random subset of clients
            chosen_clients = self.select_clients(seed=r)
            
            # Train a round
            updates = self.train_round(chosen_clients)

            # Aggregate the parameters
            self.model_params_dict = self.aggregate(updates)

            # Save in the student model the aggregated weights
            self.student_model.load_state_dict(copy.deepcopy(self.model_params_dict))

    if self.args.save:
        print(&#34;Saving full FDA model...&#34;)
        torch.save(self.model_params_dict, f&#39;models/{get_save_string(self.args, False)}_best_model.pth&#39;)

    # Validate on the train source
    self.eval_source()
    # Validate on train partition of the tarhet
    self.eval_train()
    # Validate on the test sets
    self.test()</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.train_round"><code class="name flex">
<span>def <span class="ident">train_round</span></span>(<span>self, clients)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the model with the datasets of multiple clients in a federated training round.</p>
<h2 id="args">Args</h2>
<p><code>clients</code>: A list of clients to train.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[tuple]</code></dt>
<dd>[(len_dataset, state_dictionary)]. Model updates gathered from the client to be aggregated.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_round(self, clients):
    &#34;&#34;&#34;
    Trains the model with the datasets of multiple clients in a federated training round.

    Args:
        `clients`: A list of clients to train.

    Returns:
        list[tuple]: [(len_dataset, state_dictionary)]. Model updates gathered from the client to be aggregated.
    &#34;&#34;&#34;
    updates = []
    
    # Increment the teacher steps counter
    self.teacher_counter += 1
    if (self.teacher_counter % self.args.teacher_step) == 0:
        # Update teacher
        print(f&#34;Updating teacher at step {self.teacher_counter}&#34;)
        self.teacher_model.load_state_dict(copy.deepcopy(self.model_params_dict))

    # Test client augmetation
    for i, c in enumerate(clients):
        print(f&#34;Client: {c.name} turn: Num. of samples: {len(c.dataset)}, ({i+1}/{len(clients)})&#34;)
        # Load student_model params
        c.model.load_state_dict(copy.deepcopy(self.student_model.state_dict()))
        # Set teacher model on the client
        c.set_teacher(self.teacher_model)
        # Reset counter for early stop
        c.early_stopper.reset_counter()
        # Start the train on the client
        update = c.train()
        updates.append(update)
    return updates</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.train_round_source"><code class="name flex">
<span>def <span class="ident">train_round_source</span></span>(<span>self, client)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains a single round of the source model.</p>
<h2 id="args">Args</h2>
<p><code><a title="client" href="client.html">client</a></code>: A client object containing the source dataset.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>(len_dataset, dict_params). Model updates gathered from the client to be aggregated.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_round_source(self, client):
    &#34;&#34;&#34;
    Trains a single round of the source model.

    Args:
        `client`: A client object containing the source dataset.

    Returns:
        tuple: (len_dataset, dict_params). Model updates gathered from the client to be aggregated.
    &#34;&#34;&#34;
    print(f&#34;\n Training on source dataset starting.. Num. of samples: {len(client[0].dataset)}&#34;)
    # Apply style augmentation
    client[0].set_set_style_tf_fn(self.styleaug)
    #Update parameters of the client model
    client[0].model.load_state_dict(self.model_params_dict)
    # Temp line. setup train
    self.metrics[&#34;eval_train&#34;].reset()  
    update = client[0].train(self.metrics[&#34;eval_train&#34;], [self.test_clients[0].test_loader])
    return update</code></pre>
</details>
</dd>
<dt id="fda_server.FdaServer.train_source"><code class="name flex">
<span>def <span class="ident">train_source</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the source model on the source dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_source(self):
    &#34;&#34;&#34;
    Trains the source model on the source dataset.
    &#34;&#34;&#34;
    retrain_error=False

    if self.args.load or self.args.resume or self.args.load_from:
        # If specified a custom name for the saved model load the path
        if self.args.load_from:
            pth = self.args.load_from
        else:
            pth = f&#34;models/checkpoints/{get_save_string(self.args, True)}_checkpoint.pth&#34; if self.args.chp else f&#34;models/{get_save_string(self.args, True)}_best_model.pth&#34;
        try:
            saved_params = torch.load(pth)
            self.model_params_dict = saved_params
            self.source_model.load_state_dict(saved_params)
            self.source_model.eval()
            to_print = &#34; from checkpoints.&#34; if self.args.chp else &#34;.&#34;
            print(f&#34;Source model loaded{to_print}&#34;)
        except:
            print(&#34;The checkpoint for this model does not exist. The model will be retrained.&#34;)
            retrain_error=True
            
    if (not self.args.load and not self.args.load_from) or self.args.resume or retrain_error:
        _, model_dict = self.train_round_source(self.source_dataset)
        self.model_params_dict = model_dict
        self.source_model.load_state_dict(self.model_params_dict)

    if self.args.save:
        print(&#34;Saving training source...&#34;)
        torch.save(self.model_params_dict, f&#39;models/{get_save_string(self.args, True)}_best_model.pth&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fda_server.FdaServer" href="#fda_server.FdaServer">FdaServer</a></code></h4>
<ul class="two-column">
<li><code><a title="fda_server.FdaServer.aggregate" href="#fda_server.FdaServer.aggregate">aggregate</a></code></li>
<li><code><a title="fda_server.FdaServer.eval_source" href="#fda_server.FdaServer.eval_source">eval_source</a></code></li>
<li><code><a title="fda_server.FdaServer.eval_train" href="#fda_server.FdaServer.eval_train">eval_train</a></code></li>
<li><code><a title="fda_server.FdaServer.extract_styles" href="#fda_server.FdaServer.extract_styles">extract_styles</a></code></li>
<li><code><a title="fda_server.FdaServer.predict" href="#fda_server.FdaServer.predict">predict</a></code></li>
<li><code><a title="fda_server.FdaServer.select_clients" href="#fda_server.FdaServer.select_clients">select_clients</a></code></li>
<li><code><a title="fda_server.FdaServer.test" href="#fda_server.FdaServer.test">test</a></code></li>
<li><code><a title="fda_server.FdaServer.train" href="#fda_server.FdaServer.train">train</a></code></li>
<li><code><a title="fda_server.FdaServer.train_round" href="#fda_server.FdaServer.train_round">train_round</a></code></li>
<li><code><a title="fda_server.FdaServer.train_round_source" href="#fda_server.FdaServer.train_round_source">train_round_source</a></code></li>
<li><code><a title="fda_server.FdaServer.train_source" href="#fda_server.FdaServer.train_source">train_source</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>