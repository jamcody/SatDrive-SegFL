<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>main API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import os
import random
import timeit

import numpy as np
import torch
from torch import nn
from torchvision.models import resnet18
from torchvision.transforms import RandomApply
from transformers import SegformerForSemanticSegmentation
from transformers.utils import logging

import datasets.np_transforms as nptr
import datasets.ss_transforms as sstr
import datasets.weather as weather
from client import Client
from datasets.gta5 import GTA5Dataset
from datasets.idda import IDDADataset
from datasets.loveda import LoveDADataset
from fda_server import FdaServer
from models.bisenetv2 import BiSeNetV2
from models.deeplabv3 import deeplabv3_mobilenetv2
from server import Server
from utils.args import get_parser
from utils.stream_metrics import StreamClsMetrics, StreamSegMetrics
from utils.utils import split_list_balanced, split_list_random


def set_seed(random_seed):
    random.seed(random_seed)
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def get_dataset_num_classes(dataset):
    if dataset == &#39;idda&#39;:
        return 16
    if dataset == &#39;gta5&#39;:
        return 20
    if dataset == &#39;loveda&#39;:
        return 8
    raise NotImplementedError

def model_init(args):
    &#34;&#34;&#34; Get the model based on the value of args. &#34;&#34;&#34;
    if args.model == &#39;deeplabv3_mobilenetv2&#39;:
        return deeplabv3_mobilenetv2(num_classes=get_dataset_num_classes(args.dataset))
    if args.model == &#39;resnet18&#39;:
        model = resnet18()
        model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        model.fc = nn.Linear(in_features=512, out_features=get_dataset_num_classes(args.dataset))
        return model
    if args.model == &#39;segformer&#39;:
        logging.set_verbosity(logging.FATAL)
        weights = args.transformer_model
        return SegformerForSemanticSegmentation.from_pretrained(
            f&#34;nvidia/mit-{weights}&#34;,
            num_labels=get_dataset_num_classes(args.dataset),
            ignore_mismatched_sizes=True,
        )
    if args.model == &#34;bisenetv2&#34;:
            return BiSeNetV2(get_dataset_num_classes(args.dataset), pretrained=True)

    raise NotImplementedError

def get_transforms(args):
    &#34;&#34;&#34; Get the transformations based both on the dataset and the model. &#34;&#34;&#34;
    if args.model in [&#34;segformer&#34;,&#39;deeplabv3_mobilenetv2&#39;, &#39;bisenetv2&#39;]:
        if args.dataset == &#34;loveda&#34;:
            train_transforms = sstr.Compose([
                sstr.RandomCrop((512, 512)),
                sstr.ToTensor(),
                sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            train_transforms = [
                sstr.Compose([
                    RandomApply([sstr.Lambda(lambda x: weather.add_rain(x))], p=0.15),
                ]),
                sstr.Compose([
                    sstr.RandomCrop((512, 928 if args.model != &#34;segformer&#34; else 512)),
                    sstr.ToTensor(),
                    sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ]), 
            ]
        test_transforms = sstr.Compose([
            sstr.ToTensor(),
            sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    elif args.model == &#39;resnet18&#39;:
        train_transforms = nptr.Compose([
            nptr.ToTensor(),
            nptr.Normalize((0.5,), (0.5,)),
        ])
        test_transforms = nptr.Compose([
            nptr.ToTensor(),
            nptr.Normalize((0.5,), (0.5,)),
        ])
    else:
        raise NotImplementedError
    return train_transforms, test_transforms

def get_datasets(args):
    &#34;&#34;&#34; Function to get the datasets based on the args. It return three lists: train datasets, test_datasets and if necessary
     validation datasets. &#34;&#34;&#34;
    train_datasets = []
    train_transforms, test_transforms = get_transforms(args)

    if args.dataset == &#39;idda&#39;:
        root = &#39;data/idda&#39;

        if args.centr:
          # If centralized we get all training data on one single client
          print(&#34;Centralized mode set&#34;)
          with open(os.path.join(root, &#39;train.txt&#39;), &#39;r&#39;) as f:
            all_data = f.read().splitlines()
          train_datasets.append(IDDADataset(root=root, list_samples=all_data, transform=train_transforms,
                                             client_name=&#39;centralized&#39;))
        else:
          # Otherwise we divide data in multiple datasets.
          print(&#34;Distributed Mode Set&#34;)

          with open(os.path.join(root, &#39;train.json&#39;), &#39;r&#39;) as f:
            all_data = json.load(f)
          for client_id in all_data.keys():
            train_datasets.append(IDDADataset(root=root, list_samples=all_data[client_id], transform=train_transforms,
                                              client_name=client_id))
        with open(os.path.join(root, &#39;test_same_dom.txt&#39;), &#39;r&#39;) as f:
            test_same_dom_data = f.read().splitlines()
            test_same_dom_dataset = IDDADataset(root=root, list_samples=test_same_dom_data, transform=test_transforms,
                                                client_name=&#39;test_same_dom&#39;)
        with open(os.path.join(root, &#39;test_diff_dom.txt&#39;), &#39;r&#39;) as f:
            test_diff_dom_data = f.read().splitlines()
            test_diff_dom_dataset = IDDADataset(root=root, list_samples=test_diff_dom_data, transform=test_transforms,
                                                client_name=&#39;test_diff_dom&#39;)
        test_datasets = [test_same_dom_dataset, test_diff_dom_dataset]

    elif args.dataset == &#39;gta5&#39;:
        root = &#39;data/gta5&#39;

        # Extract all data from train.txt
        all_data_train = []
        with open(os.path.join(root, &#39;train.txt&#39;), &#39;r&#39;) as f:
            all_data_train = f.read().splitlines()
        f.close()

        print(f&#34;Total number of images to be loaded: {len(all_data_train)}&#34;)
        
        if args.centr:
            # If centralized we get all training data on one single client
            print(&#34;Centralized mode set.&#34;)
            train_datasets.append(GTA5Dataset(root=root, list_samples=all_data_train, transform=train_transforms,
                                                client_name=&#39;centralized&#39;))
        else:
            # Otherwise we divide data in multiple datasets.
            print(&#34;Distributed Mode Set.&#34;)

            total_client_splits = split_list_balanced(all_data_train, args.clients_per_round*4)
            
            for i, samples in enumerate(total_client_splits):
                train_datasets.append(GTA5Dataset(root=root, list_samples=samples, transform=train_transforms,
                                                client_name=&#34;client_&#34;+str(i)))
        root_idda = &#34;data/idda&#34;

        # Test on IDDA
        with open(os.path.join(root_idda, &#39;test_same_dom.txt&#39;), &#39;r&#39;) as f:
            test_same_dom_data = f.read().splitlines()
            test_same_dom_dataset = IDDADataset(root=root_idda, list_samples=test_same_dom_data, transform=test_transforms,
                                                client_name=&#39;test_same_dom&#39;)
        with open(os.path.join(root_idda, &#39;test_diff_dom.txt&#39;), &#39;r&#39;) as f:
            test_diff_dom_data = f.read().splitlines()
            test_diff_dom_dataset = IDDADataset(root=root_idda, list_samples=test_diff_dom_data, transform=test_transforms,
                                                client_name=&#39;test_diff_dom&#39;)
        test_datasets = [test_same_dom_dataset, test_diff_dom_dataset]
        
        # Setting up IDDA as validation set
        validation_data = []
        with open(os.path.join(root_idda, &#39;train.txt&#39;), &#39;r&#39;) as f:
            all_data = f.read().splitlines()
        validation_data.append(IDDADataset(root=root_idda, list_samples=all_data, transform=train_transforms,
                                             client_name=&#39;centralized&#39;))
        
        
        return train_datasets, test_datasets, validation_data

    elif args.dataset == &#34;loveda&#34;:
        root = &#39;data/loveda&#39;

        # Extract all data from the Urban set (train) 
        all_data_train = os.listdir(os.path.join(root, &#34;Urban&#34;, &#34;images_png&#34;))

        print(f&#34;Total number of images to be loaded: {len(all_data_train)}&#34;)
        
        if args.centr:
            # If centralized we get all training data on one single client
            print(&#34;Centralized mode set.&#34;)
            train_datasets.append(LoveDADataset(root=root, list_samples=all_data_train, folder=&#34;Urban&#34;, transform=train_transforms,
                                                client_name=&#39;centralized&#39;))
        else:
            # Otherwise we divide data in multiple datasets.
            print(&#34;Distributed Mode Set.&#34;)

            total_client_splits = split_list_balanced(all_data_train, args.clients_per_round*4)
            
            for i, samples in enumerate(total_client_splits):
                train_datasets.append(LoveDADataset(root=root, list_samples=samples, folder=&#34;Urban&#34;, transform=train_transforms,
                                                client_name=&#34;client_&#34;+str(i)))
        
        # Extract test data from the Urban2 (test same domain) 
        test_same_dom_data = os.listdir(os.path.join(root, &#34;Urban2&#34;, &#34;images_png&#34;))
        test_same_dom_dataset = LoveDADataset(root=root, list_samples=test_same_dom_data, folder=&#34;Urban2&#34;, transform=test_transforms,
                                                client_name=&#39;test_same_dom&#39;)
        
        # Extract test data from the Rural (test diff domain) 
        test_diff_dom_data = os.listdir(os.path.join(root, &#34;Rural&#34;, &#34;images_png&#34;))
        test_diff_dom_dataset = LoveDADataset(root=root, list_samples=test_diff_dom_data, folder=&#34;Rural&#34;, transform=test_transforms,
                                            client_name=&#39;test_diff_dom&#39;)
        test_datasets = [test_same_dom_dataset, test_diff_dom_dataset]
        
    else:
        raise NotImplementedError

    return train_datasets, test_datasets, None

def get_source_client(args, model):
    &#34;&#34;&#34; Function to get the clients based on the dataset. This function is only used in the fda setting. Returns None otehrwise. \n
        Args:
            `args`, `model` (pytorch)
        Returns:
            list of one clients containing the source training set. This function is needed since the &#39;gen_clients&#39; functions focuses
        on the dataset split. 
    &#34;&#34;&#34;
    train_transforms, _ = get_transforms(args)
    if args.fda:
        if args.dataset == &#34;idda&#34;: # target == idda
            root = &#39;data/gta5&#39;
            # Extract all data from train.txt
            all_data_train = []
            with open(os.path.join(root, &#39;train.txt&#39;), &#39;r&#39;) as f:
                all_data_train = f.read().splitlines()
            f.close()
            sc = Client(args, GTA5Dataset(root=root, list_samples=all_data_train, transform=train_transforms, client_name=&#39;gta5_all&#39;), model)        
        
        elif args.dataset == &#34;loveda&#34;:
            root = &#39;data/loveda&#39;
            # Extract all data from the Urban (trainset) 
            all_data_train = os.listdir(os.path.join(root, &#34;Urban&#34;, &#34;images_png&#34;))
            dataset = LoveDADataset(root=root, list_samples=all_data_train, folder=&#34;Urban&#34;, transform=train_transforms,
                                                client_name=&#39;loveda_all&#39;)
            sc = Client(args, dataset, model)
        else:
            return None
        return [sc]
    else:
        return None

def set_metrics(args):
    &#34;&#34;&#34; Get the metrics used to evaluate performance based on the task (determined by the model). &#34;&#34;&#34;

    num_classes = get_dataset_num_classes(args.dataset)
    if args.model in [&#39;deeplabv3_mobilenetv2&#39;, &#34;segformer&#34;, &#34;bisenetv2&#34;]:
        metrics = {
            &#39;eval_train&#39;: StreamSegMetrics(num_classes, &#39;eval_train&#39;),
            &#39;test_same_dom&#39;: StreamSegMetrics(num_classes, &#39;test_same_dom&#39;),
            &#39;test_diff_dom&#39;: StreamSegMetrics(num_classes, &#39;test_diff_dom&#39;)
        }
    elif args.model == &#39;resnet18&#39; or args.model == &#39;cnn&#39;:
        metrics = {
            &#39;eval_train&#39;: StreamClsMetrics(num_classes, &#39;eval_train&#39;),
            &#39;test&#39;: StreamClsMetrics(num_classes, &#39;test&#39;)
        }
    else:
        raise NotImplementedError
    return metrics

def gen_clients(args, train_datasets, test_datasets, validation_datasets, model):
    &#34;&#34;&#34; Divide the datasets in clients. &#34;&#34;&#34;

    clients = [[], [], []]
    for i, datasets in enumerate([train_datasets, test_datasets]):
        # For each dataset datasets (one for each client), create and append a client
        for ds in datasets:
            clients[i].append(Client(args, ds, model, test_client=i == 1))
    if validation_datasets:
        clients[2].append(Client(args, validation_datasets[0], model, test_client=True))
    return clients[0], clients[1], clients[2]

def main():
    # Initilizalize the parser to get all the parameters
    parser = get_parser()
    args = parser.parse_args()

    # Setting up the seed for reproducibility
    set_seed(args.seed)

    # Get the model and move it to GPU
    print(&#39;Initializing model...&#39;, end=&#34; &#34;)

    # This code requires cuda enabled.
    try:
        model = model_init(args)
        model.cuda()
        print(&#34;Model Loaded: &#34;+args.model)
    except:
        print(&#34;\FATAL: seems like you have not CUDA enabled or you did not specify a model to use. Try again.&#34;)
        exit(1)
    print(&#39;Done.&#39;)

    # Get the datasets needed.
    train_datasets, test_datasets, validation_dataset = get_datasets(args)
    print(&#39;Done.&#39;)
    source_dataset = get_source_client(args, model)

    # Get the metrics needed.
    metrics = set_metrics(args)

    # Generate the clients.
    print(&#39;Generate clients...&#39;, end=&#34; &#34;)
    train_clients, test_clients, valid_clients = gen_clients(args, train_datasets, test_datasets, validation_dataset, model)
    print(&#39;Done.&#39;)

    # Setting up the server based on the mode chosen. Two server classes are available Server/FdaServer
    print(&#39;Setup server...&#39;, end=&#34; &#34;)
    if args.fda == False:
        if args.dataset == &#34;gta5&#34;:
            server = Server(args, train_clients, test_clients, model, metrics, True, valid_clients)
        else: 
            server = Server(args, train_clients, test_clients, model, metrics)
    else:
        print(&#34;\nActivating FDA mode...\t&#34;, end=&#34;&#34;)
        server = FdaServer(args, source_dataset, train_clients, test_clients, model, metrics)
    print(&#39;Done.&#39;)

    execution_time = timeit.timeit(server.train, number=1)
    print(f&#34;Execution time: {execution_time} seconds&#34;)

    # Predict an new image if needed image saved in the root directory as image_fin (for centralized setting), fda_imagine_fin (for fda) 
    if args.pred:
        print(&#34;Predicting &#34;+args.pred)
        server.predict(args.pred)

if __name__ == &#39;__main__&#39;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="main.gen_clients"><code class="name flex">
<span>def <span class="ident">gen_clients</span></span>(<span>args, train_datasets, test_datasets, validation_datasets, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Divide the datasets in clients.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gen_clients(args, train_datasets, test_datasets, validation_datasets, model):
    &#34;&#34;&#34; Divide the datasets in clients. &#34;&#34;&#34;

    clients = [[], [], []]
    for i, datasets in enumerate([train_datasets, test_datasets]):
        # For each dataset datasets (one for each client), create and append a client
        for ds in datasets:
            clients[i].append(Client(args, ds, model, test_client=i == 1))
    if validation_datasets:
        clients[2].append(Client(args, validation_datasets[0], model, test_client=True))
    return clients[0], clients[1], clients[2]</code></pre>
</details>
</dd>
<dt id="main.get_dataset_num_classes"><code class="name flex">
<span>def <span class="ident">get_dataset_num_classes</span></span>(<span>dataset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset_num_classes(dataset):
    if dataset == &#39;idda&#39;:
        return 16
    if dataset == &#39;gta5&#39;:
        return 20
    if dataset == &#39;loveda&#39;:
        return 8
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="main.get_datasets"><code class="name flex">
<span>def <span class="ident">get_datasets</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to get the datasets based on the args. It return three lists: train datasets, test_datasets and if necessary
validation datasets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_datasets(args):
    &#34;&#34;&#34; Function to get the datasets based on the args. It return three lists: train datasets, test_datasets and if necessary
     validation datasets. &#34;&#34;&#34;
    train_datasets = []
    train_transforms, test_transforms = get_transforms(args)

    if args.dataset == &#39;idda&#39;:
        root = &#39;data/idda&#39;

        if args.centr:
          # If centralized we get all training data on one single client
          print(&#34;Centralized mode set&#34;)
          with open(os.path.join(root, &#39;train.txt&#39;), &#39;r&#39;) as f:
            all_data = f.read().splitlines()
          train_datasets.append(IDDADataset(root=root, list_samples=all_data, transform=train_transforms,
                                             client_name=&#39;centralized&#39;))
        else:
          # Otherwise we divide data in multiple datasets.
          print(&#34;Distributed Mode Set&#34;)

          with open(os.path.join(root, &#39;train.json&#39;), &#39;r&#39;) as f:
            all_data = json.load(f)
          for client_id in all_data.keys():
            train_datasets.append(IDDADataset(root=root, list_samples=all_data[client_id], transform=train_transforms,
                                              client_name=client_id))
        with open(os.path.join(root, &#39;test_same_dom.txt&#39;), &#39;r&#39;) as f:
            test_same_dom_data = f.read().splitlines()
            test_same_dom_dataset = IDDADataset(root=root, list_samples=test_same_dom_data, transform=test_transforms,
                                                client_name=&#39;test_same_dom&#39;)
        with open(os.path.join(root, &#39;test_diff_dom.txt&#39;), &#39;r&#39;) as f:
            test_diff_dom_data = f.read().splitlines()
            test_diff_dom_dataset = IDDADataset(root=root, list_samples=test_diff_dom_data, transform=test_transforms,
                                                client_name=&#39;test_diff_dom&#39;)
        test_datasets = [test_same_dom_dataset, test_diff_dom_dataset]

    elif args.dataset == &#39;gta5&#39;:
        root = &#39;data/gta5&#39;

        # Extract all data from train.txt
        all_data_train = []
        with open(os.path.join(root, &#39;train.txt&#39;), &#39;r&#39;) as f:
            all_data_train = f.read().splitlines()
        f.close()

        print(f&#34;Total number of images to be loaded: {len(all_data_train)}&#34;)
        
        if args.centr:
            # If centralized we get all training data on one single client
            print(&#34;Centralized mode set.&#34;)
            train_datasets.append(GTA5Dataset(root=root, list_samples=all_data_train, transform=train_transforms,
                                                client_name=&#39;centralized&#39;))
        else:
            # Otherwise we divide data in multiple datasets.
            print(&#34;Distributed Mode Set.&#34;)

            total_client_splits = split_list_balanced(all_data_train, args.clients_per_round*4)
            
            for i, samples in enumerate(total_client_splits):
                train_datasets.append(GTA5Dataset(root=root, list_samples=samples, transform=train_transforms,
                                                client_name=&#34;client_&#34;+str(i)))
        root_idda = &#34;data/idda&#34;

        # Test on IDDA
        with open(os.path.join(root_idda, &#39;test_same_dom.txt&#39;), &#39;r&#39;) as f:
            test_same_dom_data = f.read().splitlines()
            test_same_dom_dataset = IDDADataset(root=root_idda, list_samples=test_same_dom_data, transform=test_transforms,
                                                client_name=&#39;test_same_dom&#39;)
        with open(os.path.join(root_idda, &#39;test_diff_dom.txt&#39;), &#39;r&#39;) as f:
            test_diff_dom_data = f.read().splitlines()
            test_diff_dom_dataset = IDDADataset(root=root_idda, list_samples=test_diff_dom_data, transform=test_transforms,
                                                client_name=&#39;test_diff_dom&#39;)
        test_datasets = [test_same_dom_dataset, test_diff_dom_dataset]
        
        # Setting up IDDA as validation set
        validation_data = []
        with open(os.path.join(root_idda, &#39;train.txt&#39;), &#39;r&#39;) as f:
            all_data = f.read().splitlines()
        validation_data.append(IDDADataset(root=root_idda, list_samples=all_data, transform=train_transforms,
                                             client_name=&#39;centralized&#39;))
        
        
        return train_datasets, test_datasets, validation_data

    elif args.dataset == &#34;loveda&#34;:
        root = &#39;data/loveda&#39;

        # Extract all data from the Urban set (train) 
        all_data_train = os.listdir(os.path.join(root, &#34;Urban&#34;, &#34;images_png&#34;))

        print(f&#34;Total number of images to be loaded: {len(all_data_train)}&#34;)
        
        if args.centr:
            # If centralized we get all training data on one single client
            print(&#34;Centralized mode set.&#34;)
            train_datasets.append(LoveDADataset(root=root, list_samples=all_data_train, folder=&#34;Urban&#34;, transform=train_transforms,
                                                client_name=&#39;centralized&#39;))
        else:
            # Otherwise we divide data in multiple datasets.
            print(&#34;Distributed Mode Set.&#34;)

            total_client_splits = split_list_balanced(all_data_train, args.clients_per_round*4)
            
            for i, samples in enumerate(total_client_splits):
                train_datasets.append(LoveDADataset(root=root, list_samples=samples, folder=&#34;Urban&#34;, transform=train_transforms,
                                                client_name=&#34;client_&#34;+str(i)))
        
        # Extract test data from the Urban2 (test same domain) 
        test_same_dom_data = os.listdir(os.path.join(root, &#34;Urban2&#34;, &#34;images_png&#34;))
        test_same_dom_dataset = LoveDADataset(root=root, list_samples=test_same_dom_data, folder=&#34;Urban2&#34;, transform=test_transforms,
                                                client_name=&#39;test_same_dom&#39;)
        
        # Extract test data from the Rural (test diff domain) 
        test_diff_dom_data = os.listdir(os.path.join(root, &#34;Rural&#34;, &#34;images_png&#34;))
        test_diff_dom_dataset = LoveDADataset(root=root, list_samples=test_diff_dom_data, folder=&#34;Rural&#34;, transform=test_transforms,
                                            client_name=&#39;test_diff_dom&#39;)
        test_datasets = [test_same_dom_dataset, test_diff_dom_dataset]
        
    else:
        raise NotImplementedError

    return train_datasets, test_datasets, None</code></pre>
</details>
</dd>
<dt id="main.get_source_client"><code class="name flex">
<span>def <span class="ident">get_source_client</span></span>(<span>args, model)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to get the clients based on the dataset. This function is only used in the fda setting. Returns None otehrwise. </p>
<h2 id="args">Args</h2>
<p><code>args</code>, <code>model</code> (pytorch)</p>
<h2 id="returns">Returns</h2>
<p>list of one clients containing the source training set. This function is needed since the 'gen_clients' functions focuses
on the dataset split.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_source_client(args, model):
    &#34;&#34;&#34; Function to get the clients based on the dataset. This function is only used in the fda setting. Returns None otehrwise. \n
        Args:
            `args`, `model` (pytorch)
        Returns:
            list of one clients containing the source training set. This function is needed since the &#39;gen_clients&#39; functions focuses
        on the dataset split. 
    &#34;&#34;&#34;
    train_transforms, _ = get_transforms(args)
    if args.fda:
        if args.dataset == &#34;idda&#34;: # target == idda
            root = &#39;data/gta5&#39;
            # Extract all data from train.txt
            all_data_train = []
            with open(os.path.join(root, &#39;train.txt&#39;), &#39;r&#39;) as f:
                all_data_train = f.read().splitlines()
            f.close()
            sc = Client(args, GTA5Dataset(root=root, list_samples=all_data_train, transform=train_transforms, client_name=&#39;gta5_all&#39;), model)        
        
        elif args.dataset == &#34;loveda&#34;:
            root = &#39;data/loveda&#39;
            # Extract all data from the Urban (trainset) 
            all_data_train = os.listdir(os.path.join(root, &#34;Urban&#34;, &#34;images_png&#34;))
            dataset = LoveDADataset(root=root, list_samples=all_data_train, folder=&#34;Urban&#34;, transform=train_transforms,
                                                client_name=&#39;loveda_all&#39;)
            sc = Client(args, dataset, model)
        else:
            return None
        return [sc]
    else:
        return None</code></pre>
</details>
</dd>
<dt id="main.get_transforms"><code class="name flex">
<span>def <span class="ident">get_transforms</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the transformations based both on the dataset and the model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_transforms(args):
    &#34;&#34;&#34; Get the transformations based both on the dataset and the model. &#34;&#34;&#34;
    if args.model in [&#34;segformer&#34;,&#39;deeplabv3_mobilenetv2&#39;, &#39;bisenetv2&#39;]:
        if args.dataset == &#34;loveda&#34;:
            train_transforms = sstr.Compose([
                sstr.RandomCrop((512, 512)),
                sstr.ToTensor(),
                sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            train_transforms = [
                sstr.Compose([
                    RandomApply([sstr.Lambda(lambda x: weather.add_rain(x))], p=0.15),
                ]),
                sstr.Compose([
                    sstr.RandomCrop((512, 928 if args.model != &#34;segformer&#34; else 512)),
                    sstr.ToTensor(),
                    sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ]), 
            ]
        test_transforms = sstr.Compose([
            sstr.ToTensor(),
            sstr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    elif args.model == &#39;resnet18&#39;:
        train_transforms = nptr.Compose([
            nptr.ToTensor(),
            nptr.Normalize((0.5,), (0.5,)),
        ])
        test_transforms = nptr.Compose([
            nptr.ToTensor(),
            nptr.Normalize((0.5,), (0.5,)),
        ])
    else:
        raise NotImplementedError
    return train_transforms, test_transforms</code></pre>
</details>
</dd>
<dt id="main.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    # Initilizalize the parser to get all the parameters
    parser = get_parser()
    args = parser.parse_args()

    # Setting up the seed for reproducibility
    set_seed(args.seed)

    # Get the model and move it to GPU
    print(&#39;Initializing model...&#39;, end=&#34; &#34;)

    # This code requires cuda enabled.
    try:
        model = model_init(args)
        model.cuda()
        print(&#34;Model Loaded: &#34;+args.model)
    except:
        print(&#34;\FATAL: seems like you have not CUDA enabled or you did not specify a model to use. Try again.&#34;)
        exit(1)
    print(&#39;Done.&#39;)

    # Get the datasets needed.
    train_datasets, test_datasets, validation_dataset = get_datasets(args)
    print(&#39;Done.&#39;)
    source_dataset = get_source_client(args, model)

    # Get the metrics needed.
    metrics = set_metrics(args)

    # Generate the clients.
    print(&#39;Generate clients...&#39;, end=&#34; &#34;)
    train_clients, test_clients, valid_clients = gen_clients(args, train_datasets, test_datasets, validation_dataset, model)
    print(&#39;Done.&#39;)

    # Setting up the server based on the mode chosen. Two server classes are available Server/FdaServer
    print(&#39;Setup server...&#39;, end=&#34; &#34;)
    if args.fda == False:
        if args.dataset == &#34;gta5&#34;:
            server = Server(args, train_clients, test_clients, model, metrics, True, valid_clients)
        else: 
            server = Server(args, train_clients, test_clients, model, metrics)
    else:
        print(&#34;\nActivating FDA mode...\t&#34;, end=&#34;&#34;)
        server = FdaServer(args, source_dataset, train_clients, test_clients, model, metrics)
    print(&#39;Done.&#39;)

    execution_time = timeit.timeit(server.train, number=1)
    print(f&#34;Execution time: {execution_time} seconds&#34;)

    # Predict an new image if needed image saved in the root directory as image_fin (for centralized setting), fda_imagine_fin (for fda) 
    if args.pred:
        print(&#34;Predicting &#34;+args.pred)
        server.predict(args.pred)</code></pre>
</details>
</dd>
<dt id="main.model_init"><code class="name flex">
<span>def <span class="ident">model_init</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the model based on the value of args.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_init(args):
    &#34;&#34;&#34; Get the model based on the value of args. &#34;&#34;&#34;
    if args.model == &#39;deeplabv3_mobilenetv2&#39;:
        return deeplabv3_mobilenetv2(num_classes=get_dataset_num_classes(args.dataset))
    if args.model == &#39;resnet18&#39;:
        model = resnet18()
        model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        model.fc = nn.Linear(in_features=512, out_features=get_dataset_num_classes(args.dataset))
        return model
    if args.model == &#39;segformer&#39;:
        logging.set_verbosity(logging.FATAL)
        weights = args.transformer_model
        return SegformerForSemanticSegmentation.from_pretrained(
            f&#34;nvidia/mit-{weights}&#34;,
            num_labels=get_dataset_num_classes(args.dataset),
            ignore_mismatched_sizes=True,
        )
    if args.model == &#34;bisenetv2&#34;:
            return BiSeNetV2(get_dataset_num_classes(args.dataset), pretrained=True)

    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="main.set_metrics"><code class="name flex">
<span>def <span class="ident">set_metrics</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the metrics used to evaluate performance based on the task (determined by the model).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_metrics(args):
    &#34;&#34;&#34; Get the metrics used to evaluate performance based on the task (determined by the model). &#34;&#34;&#34;

    num_classes = get_dataset_num_classes(args.dataset)
    if args.model in [&#39;deeplabv3_mobilenetv2&#39;, &#34;segformer&#34;, &#34;bisenetv2&#34;]:
        metrics = {
            &#39;eval_train&#39;: StreamSegMetrics(num_classes, &#39;eval_train&#39;),
            &#39;test_same_dom&#39;: StreamSegMetrics(num_classes, &#39;test_same_dom&#39;),
            &#39;test_diff_dom&#39;: StreamSegMetrics(num_classes, &#39;test_diff_dom&#39;)
        }
    elif args.model == &#39;resnet18&#39; or args.model == &#39;cnn&#39;:
        metrics = {
            &#39;eval_train&#39;: StreamClsMetrics(num_classes, &#39;eval_train&#39;),
            &#39;test&#39;: StreamClsMetrics(num_classes, &#39;test&#39;)
        }
    else:
        raise NotImplementedError
    return metrics</code></pre>
</details>
</dd>
<dt id="main.set_seed"><code class="name flex">
<span>def <span class="ident">set_seed</span></span>(<span>random_seed)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_seed(random_seed):
    random.seed(random_seed)
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="main.gen_clients" href="#main.gen_clients">gen_clients</a></code></li>
<li><code><a title="main.get_dataset_num_classes" href="#main.get_dataset_num_classes">get_dataset_num_classes</a></code></li>
<li><code><a title="main.get_datasets" href="#main.get_datasets">get_datasets</a></code></li>
<li><code><a title="main.get_source_client" href="#main.get_source_client">get_source_client</a></code></li>
<li><code><a title="main.get_transforms" href="#main.get_transforms">get_transforms</a></code></li>
<li><code><a title="main.main" href="#main.main">main</a></code></li>
<li><code><a title="main.model_init" href="#main.model_init">model_init</a></code></li>
<li><code><a title="main.set_metrics" href="#main.set_metrics">set_metrics</a></code></li>
<li><code><a title="main.set_seed" href="#main.set_seed">set_seed</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>